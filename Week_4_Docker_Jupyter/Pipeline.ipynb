{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "voluntary-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import category_encoders as cat_enc\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "noticed-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "#path = \"/Users/kreshnaa.raam/Documents/DR_Projects/FSDS/\"\n",
    "train = pd.read_csv(\"airline_delay_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convinced-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for date based features\n",
    "def date_features(data):\n",
    "    data['FlightDate'] = pd.to_datetime(data['FlightDate'])\n",
    "    data['day'] = pd.DatetimeIndex(data['FlightDate']).day.astype('category')\n",
    "    data['month'] = pd.DatetimeIndex(data['FlightDate']).month.astype('category')\n",
    "    data['year'] = pd.DatetimeIndex(data['FlightDate']).year.astype('category')\n",
    "    data = data.drop('FlightDate', axis=1)\n",
    "    data = data.drop('DepTime', axis=1)\n",
    "    return data\n",
    "\n",
    "# Apply FE on train data\n",
    "train = date_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aboriginal-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assign numeric and categorical variable list\n",
    "numerical_cols = ['Distance']\n",
    "categorical_cols = ['UniqueCarrier', 'Origin', 'Dest', 'Day_of_Week', 'year', 'month', 'day']\n",
    "\n",
    "# Store predictors and target in two different variables\n",
    "def split_train_data(data):\n",
    "    y = data['dep_delayed_15min']\n",
    "    X = data.drop('dep_delayed_15min', axis=1)\n",
    "    return X, y\n",
    "\n",
    "X, y = split_train_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "homeless-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply train test split with 25% data for validation\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Creating ss transformer to scale the continuous numerical data with StandardScaler()\n",
    "numerical_transformer = Pipeline(steps=[('ss', StandardScaler())])\n",
    "\n",
    "# Creating categorical transformer using ordinal encoder\n",
    "categorical_transformer = Pipeline(steps=[('ordinal', cat_enc.OrdinalEncoder())])\n",
    "\n",
    "# Creating preprocess column transformer to combine the ss and ohe pipelines\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cont', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "powered-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat classifier pipeline using random forest\n",
    "classifier_pipe = Pipeline(steps=[('preprocessor', preprocess),\n",
    "                                  ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Set hyperparameter index\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=800, stop=1000, num=2)]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 20, num=3)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [10, 20]\n",
    "\n",
    "# Create hyperparameter grid for CV Search for best model\n",
    "hyper_param_grid = {'classifier__n_estimators': n_estimators,\n",
    "                    'classifier__max_depth': max_depth,\n",
    "                    'classifier__min_samples_split': min_samples_split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "oriental-review",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('cont',\n",
       "                                                                         Pipeline(steps=[('ss',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         ['Distance']),\n",
       "                                                                        ('cat',\n",
       "                                                                         Pipeline(steps=[('ordinal',\n",
       "                                                                                          OrdinalEncoder())]),\n",
       "                                                                         ['UniqueCarrier',\n",
       "                                                                          'Origin',\n",
       "                                                                          'Dest',\n",
       "                                                                          'Day_of_Week',\n",
       "                                                                          'year',\n",
       "                                                                          'month',\n",
       "                                                                          'day'])])),\n",
       "                                       ('classifier',\n",
       "                                        RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classifier__max_depth': [10, 15, 20],\n",
       "                         'classifier__min_samples_split': [10, 20],\n",
       "                         'classifier__n_estimators': [800, 1000]},\n",
       "             scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Set grid search using roc_auc optimization with 3 fold cv\n",
    "CV = GridSearchCV(classifier_pipe, hyper_param_grid, n_jobs=-1, scoring='roc_auc', verbose=2, cv=3)\n",
    "\n",
    "# fit the model\n",
    "CV.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "incident-avatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.98      0.90     82047\n",
      "           0       0.65      0.13      0.22     19465\n",
      "\n",
      "    accuracy                           0.82    101512\n",
      "   macro avg       0.74      0.56      0.56    101512\n",
      "weighted avg       0.79      0.82      0.77    101512\n",
      "\n",
      "Cross - Validation: 0.7298930524208348\n",
      "Validation: 0.7344859318306373\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict on validation data and generate scores\n",
    "target_names = y_validation.unique().astype(str)\n",
    "y_pred = CV.predict(X_validation)\n",
    "print(classification_report(y_validation, y_pred, target_names=target_names))\n",
    "print(\"{}{}\".format(\"Cross - Validation: \", CV.best_score_))\n",
    "print(\"{}{}\".format(\"Validation: \", CV.score(X_validation, y_validation)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "popular-reader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test df\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1e77d649e0ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reading test df'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"airline_delay_test - airline_delay_test_new.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Holdout: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Reading test df')\n",
    "test = pd.read_csv(path + \"airline_delay_test - airline_delay_test_new.csv\")\n",
    "test = date_features(test)\n",
    "X, y = split_train_data(test)\n",
    "print(\"{}{}\".format(\"Holdout: \", CV.score(X,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interior-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pickle.dump(CV, open('model_randomForest_v1.sav', 'wb'))\n",
    "\n",
    "\n",
    "with open('customf.pkl', 'wb') as pkl:\n",
    "    pickle.dump(CV, pkl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-basket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-december",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-shadow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-batman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-complexity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
